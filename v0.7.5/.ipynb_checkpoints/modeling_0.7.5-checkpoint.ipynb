{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Modeling**\n",
    "\n",
    "You can use the default atmosphere extension module xpsi/surface_radiation_field/archive/hot/blackbody.pyx as described below. To run this tutorial, you should therefore be able to simply use the default extensions that are automatically compiled when X-PSI is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.ticker import MultipleLocator, AutoLocator, AutoMinorLocator\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import cm\n",
    "\n",
    "import xpsi\n",
    "\n",
    "from xpsi.global_imports import _c, _G, _dpr, gravradius, _csq, _km, _2pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build a generative model for the data; first we build a callable object for likelihood evaluation, and then we build a callable object for prior-density evaluation.\n",
    "\n",
    "En route, we will explain why various software design choices were made during development. In some cases the conventions defined are not necessarily important for future development and indeed we expect them to be redesigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Likelihood </h2>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\< a bunch of info about the parameter space and overall structure of the likelihood object we'll build. See https://thomasedwardriley.github.io/xpsi/model_construction.html \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analysis in this notebook we consider all data D to be drawn from a joint sampling distribution whose dependency on slow model source parameters is expressed in terms of a single pulse signal. The justification for such an assumption is that we are performing parameter estimation given a synthetic data set for a model pulsar with a stable (effectively non-evolving) surface radiation field, with any quasi-periodicity arising solely from relative orbital motion of source and telescope. The synthetic data is intended to emulate detection of photons over a long observing run, after which the photon incidence events are phase-folded during a pre-processing phase.\n",
    "\n",
    "This parameter estimation excercise is not blind: we know the parameter values injected to generate the synthetic dataset we will later load into a custom container.\n",
    "\n",
    "We do not need to subclass data container, but you can if you wish. X-PSI is designed this way because there is a clear common usage pattern that can be concretely implemented whilst preserving the freedom and the scope of applicability of the source code. The container instance will be available as an underscore instance method of Signal, and thus available in a derived class where we will later write code for likelihood evaluation.\n",
    "\n",
    "Hereafter we will write our custom derived classes in the notebook itself, but in practice it is best if your derived classes are written in distinct modules within a project directory, so they can be imported by a script for use with an MPI command within a shell (because in general we want to exploit parallelism for expensive likelihood evaluations).\n",
    "\n",
    "Let us load a synthetic data set that we generated in advance, and know the fictitious exposure time for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channels for event data...\n",
      "Channels set.\n"
     ]
    }
   ],
   "source": [
    "# settings = dict(counts = np.loadtxt('../../examples/data/synthetic_realisation.dat', dtype=np.double),\n",
    "#                 channels=np.arange(20,201),\n",
    "#                 phases=np.linspace(0.0, 1.0, 33),\n",
    "#                 first=0, last=180,\n",
    "#                 exposure_time=984307.6661)\n",
    "\n",
    "settings = dict(counts = np.loadtxt('data/synthetic_realisation.dat', dtype=np.double),\n",
    "                channels=np.arange(20,201),\n",
    "                phases=np.linspace(0.0, 1.0, 33),\n",
    "                first=0, last=180,\n",
    "                exposure_time=984307.6661)\n",
    "\n",
    "data = xpsi.Data(**settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the data that we aim to model. First we define some settings and helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['text.usetex'] = False\n",
    "rcParams['font.size'] = 14.0\n",
    "\n",
    "def veneer(x, y, axes, lw=1.0, length=8):\n",
    "    \"\"\" Make the plots a little more aesthetically pleasing. \"\"\"\n",
    "    if x is not None:\n",
    "        if x[1] is not None:\n",
    "            axes.xaxis.set_major_locator(MultipleLocator(x[1]))\n",
    "        if x[0] is not None:\n",
    "            axes.xaxis.set_minor_locator(MultipleLocator(x[0]))\n",
    "    else:\n",
    "        axes.xaxis.set_major_locator(AutoLocator())\n",
    "        axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    if y is not None:\n",
    "        if y[1] is not None:\n",
    "            axes.yaxis.set_major_locator(MultipleLocator(y[1]))\n",
    "        if y[0] is not None:\n",
    "            axes.yaxis.set_minor_locator(MultipleLocator(y[0]))\n",
    "    else:\n",
    "        axes.yaxis.set_major_locator(AutoLocator())\n",
    "        axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    axes.tick_params(which='major', colors='black', length=length, width=lw)\n",
    "    axes.tick_params(which='minor', colors='black', length=int(length/2), width=lw)\n",
    "    plt.setp(axes.spines.values(), linewidth=lw, color='black')\n",
    "\n",
    "def plot_one_pulse(pulse, x, label=r'Counts', cmap=cm.magma, vmin=None, vmax=None):\n",
    "    \"\"\" Plot a pulse resolved over a single rotational cycle. \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize = (7,7))\n",
    "\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[50,1])\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax_cb = plt.subplot(gs[1])\n",
    "\n",
    "    profile = ax.pcolormesh(x,\n",
    "                             data.channels,\n",
    "                             pulse,\n",
    "                             vmin = vmin,\n",
    "                             vmax = vmax,\n",
    "                             cmap = cmap,\n",
    "                             linewidth = 0,\n",
    "                             rasterized = True)\n",
    "\n",
    "    profile.set_edgecolor('face')\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylabel(r'Channel')\n",
    "    ax.set_xlabel(r'Phase')\n",
    "\n",
    "    cb = plt.colorbar(profile,\n",
    "                      cax = ax_cb)\n",
    "\n",
    "    cb.set_label(label=label, labelpad=25)\n",
    "    cb.solids.set_edgecolor('face')\n",
    "\n",
    "    veneer((0.05, 0.2), (None, None), ax)\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_one_pulse(data.counts, data.phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Instrument**\n",
    "    \n",
    "We require a model instrument object to transform incident specific flux signals into a form which enters directly in the sampling distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomInstrument(xpsi.Instrument):\n",
    "    \"\"\" A model of the NICER telescope response. \"\"\"\n",
    "\n",
    "    def __call__(self, signal, *args):\n",
    "        \"\"\" Overwrite base just to show it is possible.\n",
    "\n",
    "        We loaded only a submatrix of the total instrument response\n",
    "        matrix into memory, so here we can simplify the method in the\n",
    "        base class.\n",
    "\n",
    "        \"\"\"\n",
    "        matrix = self.construct_matrix()\n",
    "\n",
    "        self._folded_signal = np.dot(matrix, signal)\n",
    "\n",
    "        return self._folded_signal\n",
    "\n",
    "    @classmethod\n",
    "    def from_response_files(cls, ARF, RMF, max_input, min_input=0,\n",
    "                            channel_edges=None):\n",
    "        \"\"\" Constructor which converts response files into :class:`numpy.ndarray`s.\n",
    "        :param str ARF: Path to ARF which is compatible with\n",
    "                                :func:`numpy.loadtxt`.\n",
    "        :param str RMF: Path to RMF which is compatible with\n",
    "                                :func:`numpy.loadtxt`.\n",
    "        :param str channel_edges: Optional path to edges which is compatible with\n",
    "                                  :func:`numpy.loadtxt`.\n",
    "        \"\"\"\n",
    "\n",
    "        if min_input != 0:\n",
    "            min_input = int(min_input)\n",
    "\n",
    "        max_input = int(max_input)\n",
    "\n",
    "        try:\n",
    "            ARF = np.loadtxt(ARF, dtype=np.double, skiprows=3)\n",
    "            RMF = np.loadtxt(RMF, dtype=np.double)\n",
    "            if channel_edges:\n",
    "                channel_edges = np.loadtxt(channel_edges, dtype=np.double, skiprows=3)[:,1:]\n",
    "        except:\n",
    "            print('A file could not be loaded.')\n",
    "            raise\n",
    "\n",
    "        matrix = np.ascontiguousarray(RMF[min_input:max_input,20:201].T, dtype=np.double)\n",
    "\n",
    "        edges = np.zeros(ARF[min_input:max_input,3].shape[0]+1, dtype=np.double)\n",
    "\n",
    "        edges[0] = ARF[min_input,1]; edges[1:] = ARF[min_input:max_input,2]\n",
    "\n",
    "        for i in range(matrix.shape[0]):\n",
    "            matrix[i,:] *= ARF[min_input:max_input,3]\n",
    "\n",
    "        channels = np.arange(20, 201)\n",
    "\n",
    "        return cls(matrix, edges, channels, channel_edges[20:202,-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s construct an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NICER = CustomInstrument.from_response_files(ARF = '../../examples/model_data/nicer_v1.01_arf.txt',\n",
    "#                                              RMF = '../../examples/model_data/nicer_v1.01_rmf_matrix.txt',\n",
    "#                                              max_input = 500,\n",
    "#                                              min_input = 0,\n",
    "#                                              channel_edges = '../../examples/model_data/nicer_v1.01_rmf_energymap.txt')\n",
    "# NOTE: Had to change the paths to the arf, rmf etc here to match structure of my own directory\n",
    "    \n",
    "NICER = CustomInstrument.from_response_files(ARF = 'model_data/nicer_v1.01_arf.txt',\n",
    "                                             RMF = 'model_data/nicer_v1.01_rmf_matrix.txt',\n",
    "                                             max_input = 500,\n",
    "                                             min_input = 0,\n",
    "                                             channel_edges = 'model_data/nicer_v1.01_rmf_energymap.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NICER v1.01 response matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14,7))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "veneer((25, 100), (10, 50), ax)\n",
    "\n",
    "_ = ax.imshow(NICER.matrix,\n",
    "              cmap = cm.viridis,\n",
    "              rasterized = True)\n",
    "\n",
    "ax.set_ylabel('Channel $-\\;20$')\n",
    "_ = ax.set_xlabel('Energy interval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summed over channel subset [20,200]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (7,7))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "veneer((0.1, 0.5), (50,250), ax)\n",
    "\n",
    "ax.plot((NICER.energy_edges[:-1] + NICER.energy_edges[1:])/2.0, np.sum(NICER.matrix, axis=0), 'k-')\n",
    "\n",
    "ax.set_ylabel('Effective area [cm$^{-2}$]')\n",
    "_ = ax.set_xlabel('Energy [keV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Signal**\n",
    "\n",
    "We can now combine the dataset and model instrument into a Signal object. The source code for this class has methods and attributes which simplify communication between the aforementioned model objects and another object representing our model star (created below). The surface radiation field of the model star is integrated over based on energies relayed from a Signal object based on the properties of the instrument and the dataset (which are tightly coupled).\n",
    "\n",
    "We are forced to inherit from Signal and write a method that evaluates the logarithm of the likelihood conditional on a parametrised sampling distribution for the data. There is much freedom in constructing this sampling distribution, so the design strategy for X-PSI was to leave this part of the modelling process entirely to a user, guided by a number of examples. The only condition for applicability is that the sampling distribution of the data (or of each subset) can be written in terms of a set of single count-rate pulses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xpsi.likelihoods.default_background_marginalisation import eval_marginal_likelihood\n",
    "from xpsi.likelihoods.default_background_marginalisation import precomputation\n",
    "\n",
    "class CustomSignal(xpsi.Signal):\n",
    "    \"\"\"\n",
    "\n",
    "    A custom calculation of the logarithm of the likelihood.\n",
    "    We extend the :class:`~xpsi.Signal.Signal` class to make it callable.\n",
    "    We overwrite the body of the __call__ method. The docstring for the\n",
    "    abstract method is copied.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, workspace_intervals = 1000, epsabs = 0, epsrel = 1.0e-8,\n",
    "                 epsilon = 1.0e-3, sigmas = 10.0, support = None, **kwargs):\n",
    "        \"\"\" Perform precomputation.\n",
    "\n",
    "        :params ndarray[m,2] support:\n",
    "            Prior support bounds for background count rate variables in the\n",
    "            :math:`m` instrument channels, where the lower bounds must be zero\n",
    "            or positive, and the upper bounds must be positive and greater than\n",
    "            the lower bound. Alternatively, setting the an upper bounds as\n",
    "            negative means the prior support is unbounded and the flat prior\n",
    "            density functions per channel are improper. If ``None``, the lower-\n",
    "            bound of the support for each channel is zero but the prior is\n",
    "            unbounded.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(CustomSignal, self).__init__(**kwargs)\n",
    "\n",
    "        try:\n",
    "            self._precomp = precomputation(self._data.counts.astype(np.int32))\n",
    "        except AttributeError:\n",
    "            print('Warning: No data... can synthesise data but cannot evaluate a '\n",
    "                  'likelihood function.')\n",
    "        else:\n",
    "            self._workspace_intervals = workspace_intervals\n",
    "            self._epsabs = epsabs\n",
    "            self._epsrel = epsrel\n",
    "            self._epsilon = epsilon\n",
    "            self._sigmas = sigmas\n",
    "\n",
    "            if support is not None:\n",
    "                self._support = support\n",
    "            else:\n",
    "                self._support = -1.0 * np.ones((self._data.counts.shape[0],2))\n",
    "                self._support[:,0] = 0.0\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.loglikelihood, self.expected_counts, self.background_signal = \\\n",
    "                eval_marginal_likelihood(self._data.exposure_time,\n",
    "                                          self._data.phases,\n",
    "                                          self._data.counts,\n",
    "                                          self._signals,\n",
    "                                          self._phases,\n",
    "                                          self._shifts,\n",
    "                                          self._precomp,\n",
    "                                          self._support,\n",
    "                                          self._workspace_intervals,\n",
    "                                          self._epsabs,\n",
    "                                          self._epsrel,\n",
    "                                          self._epsilon,\n",
    "                                          self._sigmas,\n",
    "                                          kwargs.get('llzero'),\n",
    "                                          allow_negative=(False, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\*\\*In the first part of this notebook we define a marginal likelihood function. That is, instead of invoking the true background model that in this case is known to us, we invoke a default treatment whereby we marginalise over a set of channel-by-channel background count-rate parameters instead.**\n",
    "\n",
    "We wrote our __call__ method as a wrapper for a extension module to improve speed. The source code for the simpler case of parameter estimation when the background model is known (see path xpsi/examples/true_background). In general, if you wish to change the model for likelihood evaluation given expected signals, you can archive the Cython extensions in, e.g., the xpsi/likelihoods directory, and compile these when X-PSI is compiled and installed (by editing the setup.py script). Alternatively, you can compile your extension elsewhere and call those compiled binaries from your custom class derived from Signal.\n",
    "\n",
    "Let’s construct and instantiate a Signal object. We must accept phase shift parameters, which are a fast nuisance parameter; this detailed in the docstring of Signal. The bounds of the background parameter have already been specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = CustomSignal(data = data,\n",
    "                        instrument = NICER,\n",
    "                        background = None,\n",
    "                        interstellar = None,\n",
    "                        workspace_intervals = 1000,\n",
    "                        cache = True,\n",
    "                        epsrel = 1.0e-8,\n",
    "                        epsilon = 1.0e-3,\n",
    "                        sigmas = 10.0,\n",
    "                        support = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Constructing a star**\n",
    "\n",
    "We now need to build our star. The basic units for building a star are:\n",
    "\n",
    "* the Spacetime class;\n",
    "* the Photosphere class;\n",
    "* the HotRegion class;\n",
    "* the Elsewhere class;\n",
    "* and four low-level user-modifiable routines for evaluation of a parametrised specific intensity model.\n",
    "For this demonstration we will assume that the surface radiation field elsewhere (other than the hot regions) can be ignored in the soft X-ray regime our model instrument is sensitive to.\n",
    "\n",
    "For more advanced modelling, we can simply write custom derived classes, and instantiate those derived classes to construct objects for our model. In particular, a common pattern will be to subclass the HotRegion class. Let’s start with the Spacetime class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>The ambient spacetime**\n",
    "\n",
    "We will assume a coordinate rotation frequency based on timing analyses of 300.0 Hz; we thus fix the coordinate rotation frequency of the star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacetime = xpsi.Spacetime.fixed_spin(300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in spacetime:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can specify bounds manually for the free parameters, and give the spin frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpsi.Spacetime#? # uncomment to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = dict(distance = (0.1, 1.0),                     # (Earth) distance\n",
    "                mass = (1.0, 3.0),                       # mass\n",
    "                radius = (3.0 * gravradius(1.0), 16.0),  # equatorial radius\n",
    "                cos_inclination = (0.0, 1.0))      # (Earth) inclination to rotation axis\n",
    "\n",
    "spacetime = xpsi.Spacetime(bounds=bounds, values=dict(frequency=300.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4> The photosphere and its constituent regions**\n",
    "    \n",
    "It is not necessary for us to write a custom derived class for the photosphere object, so we will simply instantiate a Photosphere object. However, we first need an instance of HotRegion to instantiate the photosphere, and we need to implement a low-level parametrised model for the specific intensity emergent from the photosphere in a local comoving frame.\n",
    "\n",
    "The neutron star atmosphere is assumed to be geometrically thin. In the applications thus far, the surface local-comoving-frame radiation field as being described by a single free parameter: the effective temperature. The radiation field is also dependent on the local effective gravitational acceleration, however this is a derived parameter in the model. The parametrised radiation field as a function of energy and angle subtended to the normal to the (plane-parallel) atmosphere in a local comoving frame is provided as numerical model data for multi-dimensional interpolation.\n",
    "\n",
    "In X-PSI, integration over the surface radiation field is performed via calls to low-level C routines. To reduce likelihood evaluation times, the atmosphere interpolator is written in C, and calls to that interpolator are from C routine. In other words, in X-PSI, we do not use Python callback functions for evaluation of specific intensities, but C functions which are compiled when the X-PSI package is built. Unfortunately this means that to change the parametrised surface radiation field you need to get your hands a little dirty; on the bright side, the body of these functions can be implemented almost completely in the Cython language, so syntactically there is some similarity to Python because the language syntax is somewhat of a hybrid/superset. Beware, however, that the body of these functions must not contain calls to the Python API, and only to external C libraries if required: the code must evaluate to pure C, and not require the Python/C API. Note that the Python global interpreter lock is deactivated during integration to enable OpenMP multi-threading in some applications of the integrator; thus the code needs to be thread safe and nogil (not require the global interpreter lock, although a context manager could in principle be used to reacquire the lock within the integrator). Also note that if external C libraries are required, that you include a Cython .pxd (header) file in the package which externs the required library components; the library also needs to be included and linked in setup.py at package build-time.\n",
    "\n",
    "You are encouraged to ask the author of X-PSI for assistance in implementing your low-level surface radiation field model if you are uncertain. If you have ideas for making this model specification more user-friendly, without, crucially, increasing signal integration time, please contact the author or submit a pull request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \\< contents of the hot.pxd file which the X-PSI integrators use as the header file for including other C functions in the package. \\> ---\n",
    "\n",
    "You are free to modify these functions in the associated hot.pyx implementation file, and you have almost complete control over the function bodies, but not the signatures. By default the package includes an isotropic blackbody model:\n",
    "\n",
    "In most use-cases we need to modify these functions to enable handling of the numerical atmosphere data. An extension for such a case may be found as an example, which contains the extension used by Riley et al. (2019) to implement a numerical atmosphere generated by the NSX atmosphere code (Ho, W.C.G & Lai, D. 2001; Ho, W.C.G & Heinke, C.O. 2009), courtesy of W.C.G. Ho for NICER modeling efforts. A fully-ionized hydrogen atmosphere (Ho & Lai 2001) was used in Riley et al. (2019); also see Bogdanov et al. (2020).\n",
    "\n",
    "In general, if you wish to change the model for the parametrised surface local-comoving-frame radiation field model, you can archive the extensions in, e.g., the xpsi/surface_radiation_field/archive, and completely replace the contents of xpsi/surface_radiation_field/hot.pyx when X-PSI is compiled and installed. Alternatively, you can compile your extension elsewhere and link it when X-PSI is installed (by editing the setup.py script), cimporting or externing from the appropriate .pxd header file(s), and calling those precompiled binaries from the functions declared in the xpsi/surface_radiation_field/hot.pxd header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now instantiate hot region objects. We can find the required and optional parameter names in the class docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xpsi.HotRegion#? # uncomment to query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names can also be found as class attributes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpsi.HotRegion.required_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition if no custom specification means that this name is required if we do not supply custom parameters for the radiation field in the superseding member of the hot region. If we supply custom parameters, we also need to subclass xpsi.HotRegion and overwrite the \\_\\_compute_cellParamVecs method to handle our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpsi.HotRegion.optional_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of illustration, we tie the temperatures of the hot regions together: this is where the model deviates from the examples directory as alluded to in the preamble of this tutorial. There is more than one way to achieve this, but we will use the most powerful approach.*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = dict(super_colatitude = (None, None),\n",
    "              super_radius = (None, None),\n",
    "              phase_shift = (-0.25, 0.75),\n",
    "              super_temperature = (None, None))\n",
    "\n",
    "# a simple circular, simply-connected spot\n",
    "primary = xpsi.HotRegion(bounds=bounds,\n",
    "                            values={}, # no initial values and no derived/fixed\n",
    "                            symmetry=True,\n",
    "                            omit=False,\n",
    "                            cede=False,\n",
    "                            concentric=False,\n",
    "                            sqrt_num_cells=32,\n",
    "                            min_sqrt_num_cells=10,\n",
    "                            max_sqrt_num_cells=64,\n",
    "                            num_leaves=100,\n",
    "                            num_rays=200,\n",
    "                            prefix='p') # unique prefix needed because >1 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the atmospheric local-comoving-frame effective temperature is uniform everywhere within the hot region boundaries, we can use the default value of the symmetry keyword, True. All other arguments determine the numerical resolution, and have defaults which have been (somewhat arbitrarily) chosen to be result in a likelihood evaluation time of O(1) s.\n",
    "\n",
    "Let’s take a look at the xpsi.Derive docstring for guidance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpsi.Derive#? # uncomment to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class derive(xpsi.Derive):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        We can pass a reference to the primary here instead\n",
    "        and store it as an attribute if there is risk of\n",
    "        the global variable changing.\n",
    "\n",
    "        This callable can for this simple case also be\n",
    "        achieved merely with a function instead of a magic\n",
    "        method associated with a class.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __call__(self, boundto, caller = None):\n",
    "        # one way to get the required reference\n",
    "        global primary # unnecessary, but for clarity\n",
    "        return primary['super_temperature'] - 0.2\n",
    "\n",
    "bounds['super_temperature'] = None # declare fixed/derived variable\n",
    "\n",
    "secondary = xpsi.HotRegion(bounds=bounds, # can otherwise use same bounds\n",
    "                              values={'super_temperature': derive()}, # create a callable value\n",
    "                              symmetry=True,\n",
    "                              omit=False,\n",
    "                              cede=False,\n",
    "                              concentric=False,\n",
    "                              sqrt_num_cells=32,\n",
    "                              min_sqrt_num_cells=10,\n",
    "                              max_sqrt_num_cells=100,\n",
    "                              num_leaves=100,\n",
    "                              num_rays=200,\n",
    "                              is_antiphased=True,\n",
    "                              prefix='s') # unique prefix needed because >1 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description derived from ulterior variables means that when we lookup the value, it is calculated dynamically from the values of other (ulterior) model parameters. We clearly expect the temperature of the secondary hot region to behave in this way. A few other varibles do to because of keyword arguments passed upon instantiation of the hot regions. For example, the colatitude of the zero-radii omission and ceding regions (omit=False and cede=False) are equivalent to the colatitude of the centre of the superseding region. The azimuths are relative to the superseding region, and are thus listed as being fixed at zero azimuthal separation. If one of omit or cede was True, and concentric=True, a similar setup is performed, but with the radius of omit or cede being free, fixed (at finite value, but zero achieves the same as False for both omit and cede keyword arguments), or derived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to encapsulate the hot region instances in a container with properties expected by the Photosphere class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xpsi import HotRegions\n",
    "\n",
    "hot = HotRegions((primary, secondary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check out the hot regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot.objects[0] # 'p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot.objects[1] # 's'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of names, with the prefix, can also be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hot.objects[0]\n",
    "h.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.get_param('phase_shift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s set a value for the temperature of the primary hot region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot['p__super_temperature'] = 6.0 # equivalent to ``primary['super_temperature'] = 6.0``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s lookup the temperature of the secondary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary['super_temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No value was set explicitly for this secondary temperature: it is looked up dynamically from that of the primary hot region.\n",
    "\n",
    "Note that the following access will not work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot['s__super_temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this is because the temperature of the secondary hot region is a fixed/derived variable. Only free model parameters are merged into larger spaces. A fixed/derived variable needs to be accessed via the subspace that directly encapsulates a reference to it.\n",
    "\n",
    "We can now instantitate the photosphere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPhotosphere(xpsi.Photosphere):\n",
    "    \"\"\" Implement method for imaging.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def global_variables(self):\n",
    "\n",
    "        return np.array([self['p__super_colatitude'],\n",
    "                          self['p__phase_shift'] * _2pi,\n",
    "                          self['p__super_radius'],\n",
    "                          self['p__super_temperature'],\n",
    "                          self['s__super_colatitude'],\n",
    "                          (self['s__phase_shift'] + 0.5) * _2pi,\n",
    "                          self['s__super_radius'],\n",
    "                          self.hot.objects[1]['s__super_temperature']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photosphere = CustomPhotosphere(hot = hot, elsewhere = None,\n",
    "                                values=dict(mode_frequency = spacetime['frequency']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photosphere['mode_frequency'] == spacetime['frequency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that generally the Photosphere instance must have a prefix that matches a prefix given to a Signal instance to ensure the user achieves what they intend for likelihood evaluation; when the model defines multiple data subsets and thus multiple Signal instances, tagging the objects in this manner is a safety guard (in particular against inadvertently wasting compute resources sampling a distribution conditional on an unintended model). If there is one Photosphere and one Signal object, the prefixes can simply be none because there is no potential ambiguity.\n",
    "\n",
    "We do not model the surface radiation field elsewhere, and we thus leave the elsewhere keyword as None (the default). Elsewhere means on the surface, exterior to radiating hot regions that are typically expected to span a smaller angular extent; in the current version, the radiation from elsewhere, if explicitly computed is assumed to be time-invariant supposing the hot regions were not present. To account for radiation from elsewhere, a time-invariant signal is first computed, meaning an axisymmetric surface local-comoving-frame radiation field is assumed. The time-dependent signals from the hot regions are then computed, and modified by subtracting the specific intensity that would otherwise be generated by the surface local-comoving-frame radiation field from elsewhere (i.e., in place of the hot regions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Star**\n",
    "\n",
    "We can now combine the ambient spacetime, spacetime, and the embedded photosphere, photosphere, into a model star represented by an instance of Star. We do not need to extend this class, so we can simply construct and instantiate a star as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star = xpsi.Star(spacetime = spacetime, photospheres = photosphere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s check out the star object, which merged parameter subspaces associated with objects lower in the hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the free parameters are merged into a subspace higher in the object hierarchy. The reason for this is that there is not a clear and common pattern (at present) for accessing fixed/derived variables outside of the primary subspace to which they belong. If you try hard enough, of course, you can still get at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>A callable likelihood object**\n",
    "\n",
    "Given the objects constructed above and the relevant pre-compiled low-level code, we can now construct and instantiate a callable likelihood object. We do not need extend (via inheritance) the Likelihood class found the source code: this class simply combines all of the model objects defined above, performs some automatic operations given the properties of the those objects, and facilitates communication of those objects when it recieves a call to evaluate the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = xpsi.Likelihood(star = star, signals = signal,\n",
    "                             num_energies=128,\n",
    "                             threads=1,\n",
    "                             externally_updated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s retrieve the total number of free model parameters merged into the full parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that if you want to modify the definition of the model parameter space you should restart the process of constructing a likelihood object, instead of manipulating existing objects, for ultimate safety.** (You can also restart the kernel although if this is required it is a bug.)\n",
    "\n",
    "Let’s call the likelihood object with the true model parameter values that we injected to generate the synthetic data rendered above, omitting background parameters. Note that you can switch the phase interpolant invoked from GSL by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpsi.set_phase_interpolant('Akima')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-v0.6, the interpolant choice was 'Steffen', but the default is now 'Akima', and this notebook was executed with this default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [1.4,\n",
    "     12.5,\n",
    "     0.2,\n",
    "     math.cos(1.25),\n",
    "     0.0,\n",
    "     1.0,\n",
    "     0.075,\n",
    "     6.2,\n",
    "     0.025,\n",
    "     math.pi - 1.0,\n",
    "     0.2]\n",
    "\n",
    "likelihood.clear_cache()\n",
    "t = time.time()\n",
    "# source code changes since model was applied, so let's be a\n",
    "# bit lenient when checking the likelihood function\n",
    "likelihood.check(None, [-26713.6136777], 1.0e-6,\n",
    "                 physical_points=[p])\n",
    "\n",
    "print('time = %.3f s' % (time.time() - t))\n",
    "\n",
    "# > xpsi.set_phase_interpolant('Akima')\n",
    "# Checking likelihood and prior evaluation before commencing sampling...\n",
    "# Cannot import ``allclose`` function from NumPy.\n",
    "# Using fallback implementation...\n",
    "# Checking closeness of likelihood arrays:\n",
    "# -2.67136012e+04 | -2.67136137e+04 .....\n",
    "# Closeness evaluated.\n",
    "# Log-likelihood value checks passed on root process.\n",
    "# Checks passed.\n",
    "# time = 0.571 s\n",
    "\n",
    "# > xpsi.set_phase_interpolant('Steffen')\n",
    "# Checking likelihood and prior evaluation before commencing sampling...\n",
    "# Cannot import ``allclose`` function from NumPy.\n",
    "# Using fallback implementation...\n",
    "# Checking closeness of likelihood arrays:\n",
    "# -2.67136140e+04 | -2.67136137e+04 .....\n",
    "# Closeness evaluated.\n",
    "# Log-likelihood value checks passed on root process.\n",
    "# Checks passed.\n",
    "# time = 0.581 s\n",
    "\n",
    "# > xpsi.set_phase_interpolant('Cubic')\n",
    "# Checking likelihood and prior evaluation before commencing sampling...\n",
    "# Cannot import ``allclose`` function from NumPy.\n",
    "# Using fallback implementation...\n",
    "# Checking closeness of likelihood arrays:\n",
    "# -2.67135656e+04 | -2.67136137e+04 .....\n",
    "# Closeness evaluated.\n",
    "# Log-likelihood value checks passed on root process.\n",
    "# Checks passed. (increasing the tolerance to 1e-5)\n",
    "# time = 0.720 s (consistently slower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the secondary temperature was free, we would extend the vector p by one element, passing the injected value of 6.0:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary['super_temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External sampling software will interact with a likelihood object in this way. That is, it will pass some ordered container of parameter values: a vector. However, this vector will be ignored if the likelihood instance is told it can safely assume all parameters have been updated externally, meaning before the call is placed to likelihood.\\_\\_call\\_\\_(). This external update will typically happen during nested sampling when a call is placed to a prior object to inverse sample from the joint prior distribution. Our prior object can interact with our likelihood object outside of a sampling process, and thus we can encapsulate a reference to the parameter space in the prior instance and simply update the parameter values using easier handles (via \\_\\_getitem\\_\\_ magic) to conclude an inverse sampled procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Inspecting functionality**\n",
    "\n",
    "\n",
    "Calling the likelihood object also modified the signals property of the photosphere object. Let’s plot the signals by summing the count-rate over output instrument channels. We first define a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pulse():\n",
    "    \"\"\" Plot hot region signals before and after telescope operation. \"\"\"\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.set_ylabel('Signal [arbitrary normalisation]')\n",
    "    ax.set_xlabel('Phase [cycles]')\n",
    "\n",
    "    temp = np.sum(signal.signals[0], axis=0)\n",
    "    ax.plot(signal.phases[0], temp/np.max(temp), '-', color='k', lw=0.5)\n",
    "    temp = np.sum(signal.signals[1], axis=0)\n",
    "    ax.plot(signal.phases[1], temp/np.max(temp), '-', color='r', lw=0.5)\n",
    "\n",
    "    temp = np.sum(photosphere.signal[0][0], axis=0)\n",
    "    ax.plot(signal.phases[0], temp/np.max(temp), 'o-', color='k', lw=0.5, markersize=2)\n",
    "    temp = np.sum(photosphere.signal[1][0], axis=0)\n",
    "    ax.plot(signal.phases[1], temp/np.max(temp), 'o-', color='r', lw=0.5, markersize=2)\n",
    "\n",
    "    veneer((0.05,0.2), (0.05,0.2), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood(p, reinitialise=False)\n",
    "_ = plot_pulse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pulse-profiles with markers are the signals incident on the telescope, before operating on them with the response model. The markers, linearly spaced in phase, denote the phase resolution.\n",
    "\n",
    "The likelihood object calls the star.update method which in-turn calls the photosphere.embed method. The likelihood object then calls the photosphere.integrate method, passing the energies stored as the property signal.energies. We can do this manually if we wish to integrate signals but not calculate likelihoods. Here we sum over incident specific photon flux pulses as an approximation to integrating over energy. Note that we do not change the signal.signals traced by the solid curves without markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood['cos_inclination'] = math.cos(1.0)\n",
    "\n",
    "likelihood.externally_updated = True # declare safe to assume updates performed before call\n",
    "xpsi.ParameterSubspace.__call__(likelihood) # no vector supplied\n",
    "star.update()\n",
    "photosphere.integrate(energies=signal.energies, threads=1)\n",
    "\n",
    "_ = plot_pulse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the solid pulses without markers are unchanged from the plot a few cells above, and can be used to guide the eye to the effect of a change in Earth inclination.\n",
    "\n",
    "Below we print crude representations of the cell meshes spanning each hot region. The elements of a mesh cell-area array which are finite are not all identical: at the boundary of a hot region the proper area elements are smaller because of partial coverage by radiating material. The sum of all finite proper areas effectively equals the total proper area within a hot-region boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (14,7))\n",
    "\n",
    "gs = gridspec.GridSpec(1, 3, width_ratios=[50,50,1], wspace=0.2)\n",
    "ax = plt.subplot(gs[0])\n",
    "veneer((1,5), (1, 5), ax)\n",
    "\n",
    "# primary (lower colatitude) hot region\n",
    "h = hot.objects[0]\n",
    "z = h._HotRegion__cellArea[0]/np.max(h._HotRegion__cellArea[0])\n",
    "patches = plt.pcolormesh(z,\n",
    "                         vmin = np.min(z),\n",
    "                         vmax = np.max(z),\n",
    "                         cmap = cm.magma,\n",
    "                         linewidth = 1.0,\n",
    "                         rasterized = True,\n",
    "                         edgecolor='black')\n",
    "\n",
    "ax = plt.subplot(gs[1])\n",
    "veneer((1,5), (1, 5), ax)\n",
    "\n",
    "# secondary (higher colatitude) hot region\n",
    "h = hot.objects[1]\n",
    "z = h._HotRegion__cellArea[0]/np.max(h._HotRegion__cellArea[0])\n",
    "_ = plt.pcolormesh(z,\n",
    "                   vmin = np.min(z),\n",
    "                   vmax = np.max(z),\n",
    "                   cmap = cm.magma,\n",
    "                   linewidth = 1.0,\n",
    "                   rasterized = True,\n",
    "                   edgecolor='black')\n",
    "\n",
    "ax_cb = plt.subplot(gs[2])\n",
    "cb = plt.colorbar(patches,\n",
    "                  cax = ax_cb,\n",
    "                  ticks = MultipleLocator(0.2))\n",
    "\n",
    "cb.set_label(label = r'cell area (normalised by maximum)', labelpad=25)\n",
    "cb.solids.set_edgecolor('face')\n",
    "\n",
    "veneer((None, None), (0.05, None), ax_cb)\n",
    "cb.outline.set_linewidth(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the lowest colatitude row is at zero on the y-axis.\n",
    "\n",
    "Let’s plot a pulse in two dimensions. Also note that we can interpolate the signal in phase as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xpsi.tools import phase_interpolator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_pulse(z, x, shift, y, ylabel,\n",
    "                  num_rotations=5.0, res=5000, figsize=(12,6),\n",
    "                  cm=cm.viridis):\n",
    "    \"\"\" Helper function to plot a phase-energy pulse.\n",
    "\n",
    "    :param array-like z:\n",
    "        A pair of *ndarray[m,n]* objects representing the signal at\n",
    "        *n* phases and *m* values of an energy variable.\n",
    "\n",
    "    :param ndarray[n] x: Phases the signal is resolved at.\n",
    "\n",
    "    :param tuple shift: Hot region phase parameters.\n",
    "\n",
    "    :param ndarray[m] x: Energy values the signal is resolved at.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize = figsize)\n",
    "\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[50,1], wspace=0.025)\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax_cb = plt.subplot(gs[1])\n",
    "\n",
    "    new_phases = np.linspace(0.0, num_rotations, res)\n",
    "\n",
    "    interpolated = phase_interpolator(new_phases,\n",
    "                                      x,\n",
    "                                      z[0], shift[0])\n",
    "    interpolated += phase_interpolator(new_phases,\n",
    "                                       x,\n",
    "                                       z[1], shift[1])\n",
    "\n",
    "    profile = ax.pcolormesh(new_phases,\n",
    "                             y,\n",
    "                             interpolated/np.max(interpolated),\n",
    "                             cmap = cm,\n",
    "                             linewidth = 0,\n",
    "                             rasterized = True)\n",
    "\n",
    "    profile.set_edgecolor('face')\n",
    "\n",
    "    ax.set_xlim([0.0, num_rotations])\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(r'Phase')\n",
    "    veneer((0.1, 0.5), (None,None), ax)\n",
    "\n",
    "    cb = plt.colorbar(profile,\n",
    "                      cax = ax_cb,\n",
    "                      ticks = MultipleLocator(0.2))\n",
    "\n",
    "    cb.set_label(label=r'Signal (normalised by maximum)', labelpad=25)\n",
    "    cb.solids.set_edgecolor('face')\n",
    "\n",
    "    veneer((None, None), (0.05, None), ax_cb)\n",
    "    cb.outline.set_linewidth(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incident specific flux signal, in units of photons/cm2/s/keV as output by the source code, and then normalised to the maximum in specific flux:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2D_pulse((photosphere.signal[0][0], photosphere.signal[1][0]),\n",
    "              x=signal.phases[0],\n",
    "              shift=signal.shifts,\n",
    "              y=signal.energies,\n",
    "              ylabel=r'Energy (keV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count rate signal in each channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2D_pulse((signal.signals[0], signal.signals[1]),\n",
    "              x=signal.phases[0],\n",
    "              shift=signal.shifts,\n",
    "              y=NICER.channels,\n",
    "              ylabel=r'Channels',\n",
    "              cm=cm.magma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we increase the phase resolution, and plot a single rotational pulse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in hot.objects:\n",
    "    obj.set_phases(num_leaves = 1024)\n",
    "# the current relationship between objects requires that we reinitialise\n",
    "# if we wish to automatically communicate the updated settings between objects\n",
    "p[3] = math.cos(1.0) # set inclination\n",
    "_ = likelihood(p, reinitialise = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that reinitialisation also returned the following to default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.externally_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_2D_pulse((signal.signals[0], signal.signals[1]),\n",
    "              x=signal.phases[0],\n",
    "              shift=signal.shifts,\n",
    "              y=NICER.channels,\n",
    "              ylabel=r'Channels',\n",
    "              num_rotations=1.0,\n",
    "              cm=cm.magma,\n",
    "              figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s iterate over a monotonically increasing set of values of the hot-region angular radius. Note that we use the keyword threads to directly instruct the low-level routines how many OpenMP threads to spawn to accelerate the computation. Usually the likelihood object instructs the low-level routines how many threads to spawn, based on it’s thread property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we are not currently using the likelihood object as a callback function passed to posterior sampling software (which parallelises efficiently using MPI), we can safely spawn additional OpenMP threads for signal integration; if likelihood evaluations are parallelised in an MPI environment on the other hand, one risks losing efficiency by spawning OpenMP threads for likelihood evaluation.\n",
    "\n",
    "For objects that derive from ParameterSubspace we can get the current parameter vector in several ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star() == star.vector # possible shortcut to save some characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Likelihood subclass overrides the __call__ dunder, however, so we have to access it in the following ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super(xpsi.Likelihood, likelihood).__call__() == likelihood.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not define any other parameters other than those associated with the star, and thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(likelihood) == len(star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s play with some geometric parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(7,7))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.set_ylabel('photons/cm$^2$/s/keV (normalised by maxima)')\n",
    "# ax.set_xlabel('Phase [cycles]')\n",
    "\n",
    "# for obj in hot.objects:\n",
    "#     obj.set_phases(num_leaves = 256)\n",
    "\n",
    "# # let's play with the angular radius of the primary hot region\n",
    "# angular_radii = np.linspace(0.01, 1.0, 10)\n",
    "\n",
    "# likelihood.externally_updated = True\n",
    "\n",
    "# likelihood['cos_inclination'] = math.cos(1.25)\n",
    "\n",
    "# for angular_radius in angular_radii:\n",
    "#     likelihood['p__super_radius'] = angular_radius # play time\n",
    "#     star.update()\n",
    "#     photosphere.integrate(energies=signal.energies, threads=3)\n",
    "#     temp = np.sum(photosphere.signal[0][0] + photosphere.signal[1][0], axis=0)\n",
    "#     _ = ax.plot(hot.phases_in_cycles[0], temp/np.max(temp), 'k-', lw=0.5)\n",
    "\n",
    "# likelihood['cos_inclination'] = math.cos(1.0)\n",
    "\n",
    "# for angular_radius in angular_radii:\n",
    "#     likelihood['p__super_radius'] = angular_radius\n",
    "#     star.update()\n",
    "#     photosphere.integrate(energies=signal.energies, threads=3)\n",
    "#     temp = np.sum(photosphere.signal[0][0] + photosphere.signal[1][0], axis=0)\n",
    "#     _ = ax.plot(hot.phases_in_cycles[0], temp/np.max(temp), 'r-', lw=0.5)\n",
    "\n",
    "# likelihood['cos_inclination'] = math.cos(0.5)\n",
    "\n",
    "# for angular_radius in angular_radii:\n",
    "#     likelihood['p__super_radius'] = angular_radius\n",
    "#     star.update()\n",
    "#     photosphere.integrate(energies=signal.energies, threads=3)\n",
    "#     temp = np.sum(photosphere.signal[0][0] + photosphere.signal[1][0], axis=0)\n",
    "#     _ = ax.plot(hot.phases_in_cycles[0], temp/np.max(temp), 'b-', lw=0.5)\n",
    "\n",
    "# veneer((0.05,0.2), (0.05,0.2), ax)\n",
    "\n",
    "# NOTE: Commented out because it takes a while to run #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Prior**\n",
    "\n",
    "Let us now construct a callable object representing a joint prior density distribution on the space Rd. We need to extend the base class to implement our distribution, which with respect to some parameters is separable, but for others it is uniform on a joint space, and compactly supported according to non-trivial constraint equations.\n",
    "\n",
    "As an example gravitational mass and equatorial radius: a joint constraint is imposed to assign zero density to stars which are too compact: the polar radius, in units of the gravitational radius, of the rotationally deformed stellar 2-surface is too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Custom subclass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPrior(xpsi.Prior):\n",
    "    \"\"\" A custom (joint) prior distribution.\n",
    "\n",
    "    Source: Fictitious\n",
    "    Model variant: ST-U\n",
    "        Two single-temperature, simply-connected circular hot regions with\n",
    "        unshared parameters.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __derived_names__ = ['compactness', 'phase_separation',]\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Nothing to be done.\n",
    "\n",
    "        A direct reference to the spacetime object could be put here\n",
    "        for use in __call__:\n",
    "\n",
    "        .. code-block::\n",
    "\n",
    "            self.spacetime = ref\n",
    "\n",
    "        Instead we get a reference to the spacetime object through the\n",
    "        a reference to a likelihood object which encapsulates a\n",
    "        reference to the spacetime object.\n",
    "\n",
    "        \"\"\"\n",
    "        super(CustomPrior, self).__init__() # not strictly required if no hyperparameters\n",
    "\n",
    "    def __call__(self, p = None):\n",
    "        \"\"\" Evaluate distribution at ``p``.\n",
    "\n",
    "        :param list p: Model parameter values.\n",
    "\n",
    "        :returns: Logarithm of the distribution evaluated at ``p``.\n",
    "\n",
    "        \"\"\"\n",
    "        temp = super(CustomPrior, self).__call__(p)\n",
    "        if not np.isfinite(temp):\n",
    "            return temp\n",
    "\n",
    "        # based on contemporary EOS theory\n",
    "        if not self.parameters['radius'] <= 16.0:\n",
    "            return -np.inf\n",
    "\n",
    "        ref = self.parameters.star.spacetime # shortcut\n",
    "\n",
    "        # limit polar radius to try to exclude deflections >= \\pi radians\n",
    "        # due to oblateness this does not quite eliminate all configurations\n",
    "        # with deflections >= \\pi radians\n",
    "        R_p = 1.0 + ref.epsilon * (-0.788 + 1.030 * ref.zeta)\n",
    "        if R_p < 1.76 / ref.R_r_s:\n",
    "            return -np.inf\n",
    "\n",
    "        # polar radius at photon sphere for ~static star (static ambient spacetime)\n",
    "        #if R_p < 1.5 / ref.R_r_s:\n",
    "        #    return -np.inf\n",
    "\n",
    "        mu = math.sqrt(-1.0 / (3.0 * ref.epsilon * (-0.788 + 1.030 * ref.zeta)))\n",
    "\n",
    "        # 2-surface cross-section have a single maximum in |z|\n",
    "        # i.e., an elliptical surface; minor effect on support, if any,\n",
    "        # for high spin frequenies\n",
    "        if mu < 1.0:\n",
    "            return -np.inf\n",
    "\n",
    "        ref = self.parameters # redefine shortcut\n",
    "\n",
    "        # enforce order in hot region colatitude\n",
    "        if ref['p__super_colatitude'] > ref['s__super_colatitude']:\n",
    "            return -np.inf\n",
    "\n",
    "        phi = (ref['p__phase_shift'] - 0.5 - ref['s__phase_shift']) * _2pi\n",
    "\n",
    "        ang_sep = xpsi.HotRegion.psi(ref['s__super_colatitude'],\n",
    "                                     phi,\n",
    "                                     ref['p__super_colatitude'])\n",
    "\n",
    "        # hot regions cannot overlap\n",
    "        if ang_sep < ref['p__super_radius'] + ref['s__super_radius']:\n",
    "            return -np.inf\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def inverse_sample(self, hypercube=None):\n",
    "        \"\"\" Draw sample uniformly from the distribution via inverse sampling. \"\"\"\n",
    "\n",
    "        to_cache = self.parameters.vector\n",
    "\n",
    "        if hypercube is None:\n",
    "            hypercube = np.random.rand(len(self))\n",
    "\n",
    "        # the base method is useful, so to avoid writing that code again:\n",
    "        _ = super(CustomPrior, self).inverse_sample(hypercube)\n",
    "\n",
    "        ref = self.parameters # shortcut\n",
    "\n",
    "        idx = ref.index('distance')\n",
    "        ref['distance'] = truncnorm.ppf(hypercube[idx], -2.0, 7.0, loc=0.3, scale=0.1)\n",
    "\n",
    "        # flat priors in cosine of hot region centre colatitudes (isotropy)\n",
    "        # support modified by no-overlap rejection condition\n",
    "        idx = ref.index('p__super_colatitude')\n",
    "        a, b = ref.get_param('p__super_colatitude').bounds\n",
    "        a = math.cos(a); b = math.cos(b)\n",
    "        ref['p__super_colatitude'] = math.acos(b + (a - b) * hypercube[idx])\n",
    "\n",
    "        idx = ref.index('s__super_colatitude')\n",
    "        a, b = ref.get_param('s__super_colatitude').bounds\n",
    "        a = math.cos(a); b = math.cos(b)\n",
    "        ref['s__super_colatitude'] = math.acos(b + (a - b) * hypercube[idx])\n",
    "\n",
    "        # restore proper cache\n",
    "        for parameter, cache in zip(ref, to_cache):\n",
    "            parameter.cached = cache\n",
    "\n",
    "        # it is important that we return the desired vector because it is\n",
    "        # automatically written to disk by MultiNest and only by MultiNest\n",
    "        return self.parameters.vector\n",
    "\n",
    "    def transform(self, p, **kwargs):\n",
    "        \"\"\" A transformation for post-processing. \"\"\"\n",
    "\n",
    "        p = list(p) # copy\n",
    "\n",
    "        # used ordered names and values\n",
    "        ref = dict(zip(self.parameters.names, p))\n",
    "\n",
    "        # compactness ratio M/R_eq\n",
    "        p += [gravradius(ref['mass']) / ref['radius']]\n",
    "\n",
    "        # phase separation between hot regions\n",
    "        # first some temporary variables:\n",
    "        if ref['p__phase_shift'] < 0.0:\n",
    "            temp_p = ref['p__phase_shift'] + 1.0\n",
    "        else:\n",
    "            temp_p = ref['p__phase_shift']\n",
    "\n",
    "        temp_s = 0.5 + ref['s__phase_shift']\n",
    "\n",
    "        if temp_s > 1.0:\n",
    "            temp_s = temp_s - 1.0\n",
    "\n",
    "        # now append:\n",
    "        if temp_s >= temp_p:\n",
    "            p += [temp_s - temp_p]\n",
    "        else:\n",
    "            p += [1.0 - temp_p + temp_s]\n",
    "\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(likelihood.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now construct and instantiate a callable prior object, pass it to the likelihood object, which stores a mutual reference to itself in prior. **Note that if you have hyperparameters defined in the parameter subspace of prior, the prior needs to be passed upon instantation of the likelihood object so that the hyperparameters get merged into the global parameter space; this is not demonstrated in this tutorial.**\n",
    "\n",
    "--- This explains why the likelihood object even has a prior attribute, and why the J0030 analysis calls likelihood.prior = prior like this example does in a couple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = CustomPrior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "likelihood.prior = prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.merge(prior) # if there were hyperparameters, can merge them in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.merge(prior) # if there were hyperparameters, can merge them in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.externally_updated = True # already set above, but for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior() # a parameter vector is already stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also defined a transform method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.transform(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penultimate entry is the compactness ratio M/R_eq, which should have a familar magnitude. The last entry is the phase separation in cycles.\n",
    "\n",
    "The prior.inverse_sample() method is required by MultiNest to uniformly sample from the prior distribution and transform it into a posterior distribution. Let’s call the method, passing a vector of pseudorandom numbers drawn when each is drawn from a uniform distribution on the interval [0,1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.inverse_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, inverse sampling from the prior can be used to initialise the ensemble of walkers evolved by emcee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Density and support checking**\n",
    "\n",
    "Let’s draw samples from the prior and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samps, _ = prior.draw(int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samps(samps, x, y, xlabel, ylabel, s=1.0, color='k', **kwargs):\n",
    "    \"\"\" Plot samples as 2D scatter plot. \"\"\"\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(samps[:,x], samps[:,y], s=s, color=color, **kwargs)\n",
    "    veneer(None, None, ax)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    return plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first plot the (M,Req) samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_samps(samps,\n",
    "                likelihood.index('radius'), # we can find the index for __getitem__ magic\n",
    "                likelihood.index('mass'),\n",
    "                likelihood.get_param('radius').symbol + ' [km]', # can use symbols like so\n",
    "                r'$M$ [M$_{\\odot}$]') # or manual entry\n",
    "\n",
    "# the Schwarzschild photon sphere R_eq = 1.5 x r_s(M)\n",
    "ax.plot(3.0*gravradius(np.linspace(1.0,3.0,100)), np.linspace(1.0,3.0,100), 'k-')\n",
    "\n",
    "# R_eq = 1.76 x r_s(M)\n",
    "_ = ax.plot(2.0*1.76*gravradius(np.linspace(1.0,3.0,100)), np.linspace(1.0,3.0,100), 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the prior support is defined with a constraint that the polar radius Rp(Req,M,Ω)≥1.76rs(M), which is why there is a region devoid of samples between the prior support and the dashed line Req=1.76rs(M). This condition is not necessary though because as of v0.6, X-PSI can include higher-order images, with the secondary and tertiary expected to be entirely sufficient in practice.\n",
    "\n",
    "--- **What does he mean by higher order images? Images as in hot region shapes?**\n",
    "\n",
    "Let’s now plot the hot region (circular spot) colatitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_samps(samps,\n",
    "                likelihood.index('p__super_colatitude'),\n",
    "                likelihood.index('s__super_colatitude'),\n",
    "                r'$\\Theta_{p}$ [radians]', r'$\\Theta_{s}$ [radians]')\n",
    "\n",
    "# enforce colatitude order to distinguish hot regions as primary and secondary\n",
    "_ = ax.plot(np.array([0.0,math.pi]), np.array([0.0,math.pi]), 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the samples, marginalised over other region geometry parameters, are sparser when both hot regions approach the poles because we exclude overlapping configurations from the prior support. This is because the hot regions are by convention defined as disjoint, and cannot merge. If one wanted a more complex hot region, one would not invoke multiple hot regions that are permitted to overlap, but one would instead handle the extra complexity within the HotRegion class or a subclass.\n",
    "\n",
    "Let’s plot the angular radii of the spots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_samps(samps,\n",
    "               likelihood.index('p__super_radius'),\n",
    "               likelihood.index('s__super_radius'),\n",
    "               r'$\\zeta_{p}$ [radians]',\n",
    "               r'$\\zeta_{s}$ [radians]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the prior density is greater for hot regions that subtend smaller solid angles at the centre of the star, which also derives from the non-overlapping criterion for prior support.\n",
    "\n",
    "Finally, let’s take a look at the phases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_samps(samps,\n",
    "               likelihood.index('p__phase_shift'),\n",
    "               likelihood.index('s__phase_shift'),\n",
    "               r'$\\phi_{p}$ [cycles]',\n",
    "               r'$\\phi_{s}$ [cycles]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that again because the hot regions cannot overlap, rarefaction occurs in the vicinity of lines of minimal phase separation. Note that the boundaries are all periodic, so this pattern tesselates. Because we implemented a transformation in our CustomPrior subclass, we can actually draw the samples and transform them, which is useful in post-processing contexts. We defined the intervals [-0.25, 0.75] for the inverse sampling so that the posterior mode(s) will not be near a boundary. The nested sampling algorithm can handle periodic boundaries by defining wrapped parameters; however, this can be trivially avoided altogether by rough inspection of the phases of the subpulses in the data, which we can see above are at around −0.1 and 0.4 given the respective ground truth (injected) phases of ϕp=0.0 and ϕs=0.025.\n",
    "\n",
    "Transformations for the purpose of likelihood evaluation must be handled in the inverse_sample method of an instance of the Prior class, but additional transformations that extend the parameter vector are written in the transform method.\n",
    "\n",
    "If we wanted to transform automatically upon drawing the samples, thereby extending the parameter vectors passed to the __call__ method (so be careful with wrap-around indices when evaluating prior support conditions), we would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samps_plus_transformed, _ = prior.draw(int(1e4), transform=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a transformation from the hot region centre phases to the phase separation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_samps(samps_plus_transformed,\n",
    "               likelihood.index('p__phase_shift'),\n",
    "               likelihood.prior.index('phase_separation'),\n",
    "               r'$\\phi_{p}$ [cycles]',\n",
    "               r'$\\Delta\\phi$ [cycles]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the rarefaction occurs for Δϕ∼0.0=1.0.\n",
    "\n",
    "The marginal one-dimensional prior distributions are overplotted, by the PostProcessing module, with the posterior distributions.\n",
    "\n",
    "It is recommended to carefully inspect joint prior samples for pairs of parameters before commencing a sampling run, especially if there is a non-trivial constraint equation imposed on the prior support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Sampling interface**\n",
    "\n",
    "We have constructed and instantiated both a callable likelihood object and a callable prior object. We could proceed, for example, to apply the open-source sampler emcee to the joint posterior distribution proportional to the product of the (exponentiated) calls to the likelihood and prior objects.\n",
    "\n",
    "**<h4>Ensemble MCMC**\n",
    "\n",
    "To prove that the objects constructed above can be fed to the emcee sampler, let’s run a number of iterations using a single Python process. We will initialise the ensemble by drawing from a multivariate Gaussian with mean vector equal to the ground truth (injected) vector.\n",
    "\n",
    "**NOTE: I don't actually need emcee to work since we'll be using MultiNest like Riley did, so can prob skip this section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std = [1.0,\n",
    "#          0.05,\n",
    "#          1.0,\n",
    "#          0.01,\n",
    "#          0.05,\n",
    "#          0.0025,\n",
    "#          0.01,\n",
    "#          0.05,\n",
    "#          0.05,\n",
    "#          0.01,\n",
    "#          0.01]\n",
    "\n",
    "# runtime_params = {'resume': False,\n",
    "#                   'root_dir': './',\n",
    "#                   'nwalkers': 50,\n",
    "#                   'nsteps': 100,\n",
    "#                   'walker_dist_moments': zip(p, std)} #  if resume then ``None``\n",
    "\n",
    "# for h in hot.objects:\n",
    "#     h.set_phases(num_leaves = 100) # revert to original phase resolution above\n",
    "\n",
    "# likelihood.threads = 3\n",
    "# likelihood.reinitialise() # because we played with the object above\n",
    "\n",
    "# # Use MPI=False for testing purposes\n",
    "# backend = xpsi.Sample.ensemble(likelihood, prior,\n",
    "#                                MPI=False, **runtime_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could also try initialising the ensemble by inverse sampling the joint prior distribution.\n",
    "\n",
    "Let’s quickly plot the evolution of the ensemble Markov chains to prove that the sampling process commenced and is behaving in a somewhat reasonable manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     backend\n",
    "# except NameError:\n",
    "#     import emcee\n",
    "#     backend = emcee.backends.HDFBackend('samples.h5')\n",
    "\n",
    "# chain = backend.get_chain()\n",
    "\n",
    "# # These chains plotted were generated using\n",
    "# # v0.2 so the labels here are in a different\n",
    "# # order. The model also had two free\n",
    "# # temperature parameters instead of just one.\n",
    "# # Also, inclination was the parameter, not\n",
    "# # cos(inclination), and the prior was flat\n",
    "# # in inclination, not cos(inclination).\n",
    "# labels = [r'$D$',\n",
    "#           r'$M$',\n",
    "#           r'$R_{\\rm eq}$',\n",
    "#           r'$\\cos(i)$',\n",
    "#           r'$\\phi_{p}$',\n",
    "#           r'$\\Theta_{p}$',\n",
    "#           r'$\\zeta_{p}$',\n",
    "#           r'$T_{p}$',\n",
    "#           r'$\\phi_{s}$'\n",
    "#           r'$\\Theta_{s}$',\n",
    "#           r'$\\zeta_{s}$']\n",
    "\n",
    "# fig = plt.figure(figsize=(8,32))\n",
    "\n",
    "# gs = gridspec.GridSpec(12, 1, hspace=0.15)\n",
    "\n",
    "# for i in range(len(labels)):\n",
    "#     ax = plt.subplot(gs[i,0])\n",
    "#     ax.set_ylabel(labels[i])\n",
    "#     for j in range(50):\n",
    "#         plt.plot(chain[:,j,i], 'k-', lw=0.5, alpha=0.5)\n",
    "#     if i < 11:\n",
    "#         ax.tick_params(axis='x', labelbottom=False)\n",
    "#         plt.setp(ax.get_yticklabels()[0], visible=False)\n",
    "#         plt.setp(ax.get_yticklabels()[-1], visible=False)\n",
    "#     else: ax.set_xlabel('Steps')\n",
    "#     veneer((250, 1000), None, ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chains rendered in the documentation were run on a desktop machine in about a day of wall-time. It is visually discernable that the ensemble distribution has not yet evolved to a stationary state: a rigourous application of ensemble MCMC would cover convergence criteria, auto-correlation, and examination of sensitivity to initial conditions and the transition kernel. In fact, based on the analysis with nested sampling on path xpsi/examples/default_background, we know that the posterior mode in the vicinity of the above ensemble is rather non-linear in the space being sampled, so ensemble MCMC may require many steps in order to argue for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Nested sampling**\n",
    "\n",
    "We interface with the nested sampler MultiNest in a similar manner, by defining some runtime settings, and then passing those settings together with likelihood and prior objects to a wrapper from the Sample module. We will run the sampler for a specified number (1000) of nested replacements (iterations). Note that the output below was actually generated with the temperature of the secondary hot region as a free parameter, resulting in an additional dimension to the model above.\n",
    "\n",
    "The environment variable LD_LIBRARY_PATH must be set before launching Jupyter as follows:\n",
    "\n",
    "$ export LD_LIBRARY_PATH=<path/to/multinest>/lib\n",
    "\n",
    "Get the parameters that are periodic or wrapped to inform MultiNest of boundary properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_params = [0]*len(likelihood)\n",
    "wrapped_params[likelihood.index('p__phase_shift')] = 1\n",
    "wrapped_params[likelihood.index('s__phase_shift')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runtime_params = {'resume': False,\n",
    "                  'importance_nested_sampling': False,\n",
    "                  'multimodal': False,\n",
    "                  'n_clustering_params': None,\n",
    "                  'outputfiles_basename': './run/run', # make ./run directory manually\n",
    "                  'n_iter_before_update': 50,\n",
    "                  'n_live_points': 100,\n",
    "                  'sampling_efficiency': 0.8,\n",
    "                  'const_efficiency_mode': False,\n",
    "                  'wrapped_params': wrapped_params,\n",
    "                  'evidence_tolerance': 0.5,\n",
    "                  'max_iter': 1000, # manual termination condition for short test\n",
    "                  'verbose': True}\n",
    "\n",
    "for h in hot.objects:\n",
    "    h.set_phases(num_leaves = 100)\n",
    "\n",
    "likelihood.threads = 3\n",
    "likelihood.reinitialise()\n",
    "likelihood.clear_cache()\n",
    "\n",
    "# inform source code that parameter objects updated when inverse sampling\n",
    "likelihood.externally_updated = True\n",
    "\n",
    "p = [1.4,\n",
    "     12.5,\n",
    "     0.2,\n",
    "     math.cos(1.25),\n",
    "     0.0,\n",
    "     1.0,\n",
    "     0.075,\n",
    "     6.2,\n",
    "     0.025,\n",
    "     math.pi - 1.0,\n",
    "     0.2]\n",
    "\n",
    "# let's require that checks pass before starting to sample\n",
    "check_kwargs = dict(hypercube_points = None,\n",
    "                    physical_points = p, # externally_updated preserved\n",
    "                    loglikelihood_call_vals = [-26713.613677], # from above\n",
    "#                     rtol_loglike = 1.0e-6) # choose a tolerance --- ORIGINAL\n",
    "                    rtol_loglike = 1.0e-1) # choose a tolerance\n",
    "\n",
    "# note that mutual refs are already stored in the likelihood and prior\n",
    "# objects to facilitate communication externally of the sampling process\n",
    "xpsi.Sample.nested(likelihood, prior, check_kwargs, **runtime_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^Currently this fails for me with an error, but it should output the following:\n",
    "    \n",
    "Imported PyMultiNest.\n",
    "\n",
    "Checking likelihood and prior evaluation before commencing sampling...\n",
    "\n",
    "Cannot import ``allclose`` function from NumPy...\n",
    "\n",
    "Using fallback implementation...\n",
    "\n",
    "Log-likelihood value checks passed on root process.\n",
    "\n",
    "Checks passed.\n",
    "\n",
    "Commencing integration...\n",
    "\n",
    "Estimating fractional hypervolume of the unit hypercube with finite prior density:\n",
    "\n",
    "Requiring 1E+05 draws from the prior support for Monte Carlo estimation...\n",
    "\n",
    "Drawing samples from the joint prior...\n",
    "\n",
    "Samples drawn.\n",
    "\n",
    "The support occupies an estimated 11.9% of the hypervolume within the unit hypercube...\n",
    "\n",
    "Fractional hypervolume estimated.\n",
    "\n",
    "Sampling efficiency set to: 6.7485."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verbose output of the MultiNest program is by default directed to the host terminal session. Instead of trying to redirect that output to that of the above cell, we simply copy and paste the output from the terminal below. For the purpose of illustration, this output was for a 12-dimensional model variant (as for the emcee plot above).\n",
    "\n",
    "The verbose output looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************\n",
    "# MultiNest v3.11\n",
    "# Copyright Farhan Feroz & Mike Hobson\n",
    "# Release Apr 2018\n",
    "\n",
    "# no. of live points =  100\n",
    "\n",
    "# dimensionality =   12\n",
    "# *****************************************************\n",
    "# Starting MultiNest\n",
    "# generating live points\n",
    "#  live points generated, starting sampling\n",
    "# Acceptance Rate:                        0.724638\n",
    "# Replacements:                                100\n",
    "# Total Samples:                               138\n",
    "# Nested Sampling ln(Z):            **************\n",
    "# Acceptance Rate:                        0.649351\n",
    "# Replacements:                                150\n",
    "# Total Samples:                               231\n",
    "# Nested Sampling ln(Z):            -116670.287917\n",
    "#     .\n",
    "#     .\n",
    "#     .\n",
    "# Acceptance Rate:                        0.058427\n",
    "# Replacements:                               1050\n",
    "# Total Samples:                             17971\n",
    "# Nested Sampling ln(Z):             -28879.280235\n",
    "#  ln(ev)=  -28879.280235090871      +/-                       NaN\n",
    "#  Total Likelihood Evaluations:        17971\n",
    "#  Sampling finished. Exiting MultiNest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Synthesis**\n",
    "\n",
    "In this notebook thus far we have not generated sythetic data. However, we did condition on synthetic data. Below we outline how that data was generated.\n",
    "\n",
    "**<h4>Background**\n",
    "    \n",
    "The background radiation field incident on the model instrument for the purpose of generating synthetic data was a time-invariant powerlaw spectrum, and was transformed into a count-rate in each output channel using the response matrix for synthetic data generation. We would reproduce this background here by writing a custom subclass as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBackground(xpsi.Background):\n",
    "    \"\"\" The background injected to generate synthetic data. \"\"\"\n",
    "\n",
    "    def __init__(self, bounds=None, value=None):\n",
    "\n",
    "        # first the parameters that are fundemental to this class\n",
    "        doc = \"\"\"\n",
    "        Powerlaw spectral index.\n",
    "        \"\"\"\n",
    "        index = xpsi.Parameter('powerlaw_index',\n",
    "                                strict_bounds = (-3.0, -1.01),\n",
    "                                bounds = bounds,\n",
    "                                doc = doc,\n",
    "                                symbol = r'$\\Gamma$',\n",
    "                                value = value)\n",
    "\n",
    "        super(CustomBackground, self).__init__(index)\n",
    "\n",
    "    def __call__(self, energy_edges, phases):\n",
    "        \"\"\" Evaluate the incident background field. \"\"\"\n",
    "\n",
    "        G = self['powerlaw_index']\n",
    "\n",
    "        temp = np.zeros((energy_edges.shape[0] - 1, phases.shape[0]))\n",
    "\n",
    "        temp[:,0] = (energy_edges[1:]**(G + 1.0) - energy_edges[:-1]**(G + 1.0)) / (G + 1.0)\n",
    "\n",
    "        for i in range(phases.shape[0]):\n",
    "            temp[:,i] = temp[:,0]\n",
    "\n",
    "        self.background = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the analytic background is integrated over energy intervals, as required by a Signal instance, which would then straightforwardly apply the model instrument response to the background.\n",
    "\n",
    "We can now construct and instantiate a background object. The base clase xpsi.Background is inherited from the ParameterSubspace ABC. We therefore need to specify the number of background parameters, and define the hard bounds on those parameters; in this case we have only a single parameter, the powerlaw index.\n",
    "\n",
    "We would then instantiate as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = CustomBackground(bounds=(None, None)) # use strict bounds, but do not fix/derive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Data format**\n",
    "\n",
    "We are also in need of a simpler data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesiseData(xpsi.Data):\n",
    "    \"\"\" Custom data container to enable synthesis. \"\"\"\n",
    "\n",
    "    def __init__(self, channels, phases, first, last):\n",
    "\n",
    "        self.channels = channels\n",
    "        self._phases = phases\n",
    "\n",
    "        try:\n",
    "            self._first = int(first)\n",
    "            self._last = int(last)\n",
    "        except TypeError:\n",
    "            raise TypeError('The first and last channels must be integers.')\n",
    "        if self._first >= self._last:\n",
    "            raise ValueError('The first channel number must be lower than the '\n",
    "                             'the last channel number.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = SynthesiseData(np.arange(20,201), np.linspace(0.0, 1.0, 33), 0, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Custom method**\n",
    "\n",
    "We are in need of a synthesise method, which in this implementation wraps an extension module. Let’s check what the extension module offers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xpsi.tools.synthesise import synthesise_given_total_count_number as _synthesise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_synthesise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesise(self,\n",
    "               require_source_counts,\n",
    "               require_background_counts,\n",
    "               name='synthetic',\n",
    "               directory='./data',\n",
    "               **kwargs):\n",
    "        \"\"\" Synthesise data set.\n",
    "\n",
    "        \"\"\"\n",
    "        self._expected_counts, synthetic, _, _ = _synthesise(self._data.phases,\n",
    "                                            require_source_counts,\n",
    "                                            self._signals,\n",
    "                                            self._phases,\n",
    "                                            self._shifts,\n",
    "                                            require_background_counts,\n",
    "                                            self._background.registered_background)\n",
    "        try:\n",
    "            if not os.path.isdir(directory):\n",
    "                os.mkdir(directory)\n",
    "        except OSError:\n",
    "            print('Cannot create write directory.')\n",
    "            raise\n",
    "\n",
    "        np.savetxt(os.path.join(directory, name+'_realisation.dat'),\n",
    "                   synthetic,\n",
    "                   fmt = '%u')\n",
    "\n",
    "        self._write(self.expected_counts,\n",
    "                    filename = os.path.join(directory, name+'_expected_hreadable.dat'),\n",
    "                    fmt = '%.8e')\n",
    "\n",
    "        self._write(synthetic,\n",
    "                    filename = os.path.join(directory, name+'_realisation_hreadable.dat'),\n",
    "                    fmt = '%u')\n",
    "\n",
    "def _write(self, counts, filename, fmt):\n",
    "    \"\"\" Write to file in human readable format. \"\"\"\n",
    "\n",
    "    rows = len(self._data.phases) - 1\n",
    "    rows *= len(self._data.channels)\n",
    "\n",
    "    phases = self._data.phases[:-1]\n",
    "    array = np.zeros((rows, 3))\n",
    "\n",
    "    for i in range(counts.shape[0]):\n",
    "        for j in range(counts.shape[1]):\n",
    "            array[i*len(phases) + j,:] = self._data.channels[i], phases[j], counts[i,j]\n",
    "\n",
    "    np.savetxt(filename, array, fmt=['%u', '%.6f'] + [fmt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add unbound methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomSignal.synthesise = synthesise\n",
    "CustomSignal._write = _write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate, and reconfigure the likelihood object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = CustomSignal(data = _data,\n",
    "                        instrument = NICER,\n",
    "                        background = background,\n",
    "                        interstellar = None,\n",
    "                        prefix='NICER')\n",
    "\n",
    "for h in hot.objects:\n",
    "    h.set_phases(num_leaves = 100)\n",
    "\n",
    "likelihood = xpsi.Likelihood(star = star, signals = signal, threads=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h4>Synthesise**\n",
    "    \n",
    "We proceed to synthesise. First we set an environment variable to seed the random number generator being called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env GSL_RNG_SEED=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check write path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [1.4,\n",
    "     12.5,\n",
    "     0.2,\n",
    "     math.cos(1.25),\n",
    "     0.0,\n",
    "     1.0,\n",
    "     0.075,\n",
    "     6.2,\n",
    "     0.025,\n",
    "     math.pi - 1.0,\n",
    "     0.2,\n",
    "     -2.0]\n",
    "\n",
    "NICER_kwargs = dict(require_source_counts=2.0e6,\n",
    "                     require_background_counts=2.0e6,\n",
    "                     name='new_synthetic',\n",
    "                     directory='./data')\n",
    "\n",
    "likelihood.synthesise(p, force=True, NICER=NICER_kwargs) # SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_pulse(np.loadtxt('data/new_synthetic_realisation.dat', dtype=np.double), _data.phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check we have generated the same count numbers, given the same seed and resolution settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.counts - np.loadtxt('data/new_synthetic_realisation.dat', dtype=np.double)\n",
    "(diff != 0.0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be False if there is no difference in the expectations of the Poisson-distribution random variables, as was the case for xpsi.__version__ <= v0.5. Now however, because of slight tweaks to the signal integrators, small differences manifest, especially when hot region visibility gets truncated at the stellar limb. The major cause of this difference is because we swap to the Akima Periodic splines phase interpolants from GSL. Previously we invoked the non-periodic Steffen spline interpolants (the important feature for comparison being that minima and maxima in the interpolant can only occur at spline nodes), also from GSL. The aim in doing this is to increase accuracy near the signal maxima and minima, whilst potentially sacrificing some accuracy when interpolating a curve that is truncated at zero due to a radiating element going out of visibility, or in the context of an extended radiating region, that region going fully out of visibility.\n",
    "\n",
    "The change in the log-likelihood was small, at the ∼5×10^−5% level, but let’s plot the difference between the expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.loadtxt('../../examples/data/synthetic_expected_hreadable.dat')\n",
    "# y = np.loadtxt('data/new_synthetic_expected_hreadable.dat')\n",
    "# NOTE: Had to change directories here\n",
    "\n",
    "x = np.loadtxt('../data/synthetic_expected_hreadable.dat')\n",
    "y = np.loadtxt('data/new_synthetic_expected_hreadable.dat')\n",
    "\n",
    "xx = np.zeros(data.counts.shape)\n",
    "yy = np.zeros(data.counts.shape)\n",
    "\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        xx[i,j] = x[i*32 + j,-1]\n",
    "        yy[i,j] = y[i*32 + j,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_pulse(yy, _data.phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_pulse(yy-xx, _data.phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And normalised by the root-variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_r = (yy-xx)/np.sqrt(yy)\n",
    "plot_one_pulse(_r, _data.phases,\n",
    "               'Standardised residuals',\n",
    "               cmap=cm.RdBu,\n",
    "               vmin=-np.max(np.fabs(_r)),\n",
    "               vmax=np.max(np.fabs(_r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between realisations is far larger than one count (which is what might be expected in some cases due to discreteness of the Poisson probability mass function) for many intervals, despite the small change in the expectation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_pulse(diff, _data.phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m actually unsure why a such small changes in the Poisson-distributed random variable expectations yields such large differences in the random variates for the same GSL RNG seed. For a substantial subset of elements, the random variates are equal, and that is not always where the difference in expectations is zero or relatively small. I consider this an open problem: please get in touch if you happen to read this and you have some insight you can share!\n",
    "\n",
    "Here is the distribution of standardised variate residuals, for the realisation generated by this notebook, bearing in mind noise and that the Poisson distribution deparature from Gaussianity for lower expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "ax = plt.gcf().add_subplot(111)\n",
    "\n",
    "variates = (yy - np.loadtxt('data/new_synthetic_realisation.dat', dtype=np.double))/np.sqrt(yy)\n",
    "\n",
    "_ = ax.hist(variates.flatten(),\n",
    "             bins=50,\n",
    "             density=True,\n",
    "             histtype='step',\n",
    "             color='k',\n",
    "             lw=1.0)\n",
    "\n",
    "_x = np.linspace(-5.0,5.0,1000)\n",
    "ax.plot(_x, np.exp(-0.5 * _x**2.0)/np.sqrt(2.0 * math.pi), 'k-.')\n",
    "\n",
    "ax.set_xlabel('Count number standardised residuals')\n",
    "ax.set_ylabel('Density')\n",
    "veneer((0.25, 1.0), (0.01, 0.1), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are no discernable correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_pulse(variates, _data.phases,\n",
    "               'Standardised residuals',\n",
    "               cmap=cm.RdBu,\n",
    "               vmin=-np.max(np.fabs(variates)),\n",
    "               vmax=np.max(np.fabs(variates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we switch back to a Steffen phase spline, the issue does not entirely disappear, but the differences are seemingly too small to affect random number generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpsi.set_phase_interpolant('Steffen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.synthesise(p, force=True, NICER=NICER_kwargs) # SEED=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the small difference here in exposure time and background normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.loadtxt('data/new_synthetic_expected_hreadable.dat')\n",
    "zz = np.zeros(data.counts.shape)\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        zz[i,j] = z[i*32 + j,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_r = (zz-xx)/np.sqrt(zz)\n",
    "plot_one_pulse(_r, _data.phases,\n",
    "               'Standardised residuals',\n",
    "               cmap=cm.RdBu,\n",
    "               vmin=-np.max(np.fabs(_r)),\n",
    "               vmax=np.max(np.fabs(_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.counts - np.loadtxt('data/new_synthetic_realisation.dat', dtype=np.double)\n",
    "(diff != 0.0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
